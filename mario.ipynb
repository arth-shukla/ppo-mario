{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsu3DASF9oOZ"
      },
      "source": [
        "# DDQN to Beat Mario"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJC6m4o-bg3"
      },
      "source": [
        "## Background\n",
        "\n",
        "In 2013, DeepMind made the Deep Q Network to play Atari Games (https://arxiv.org/pdf/1312.5602.pdf). This DQN was able to learn to play simple Atari games at human level, kickstarting the past decade of DeepRL research.\n",
        "\n",
        "In 2015, DeepMind published another paper which played even more Atari Games (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). This new model included a target network to reduce Q-value overestimation in the previous DQN model. This model is sometimes called DDQNs, and it's what we'll implement here to beat Mario!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmHHVWJ91H5"
      },
      "source": [
        "## Setup and Some Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8eCfnF-Hpu"
      },
      "source": [
        "First, we install gymnasium and the Mario Env.\n",
        "\n",
        "The Mario env can be found here: https://pypi.org/project/gym-super-mario-bros/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97LB2zyA90n1",
        "outputId": "ace16065-3afb-4b63-a812-3412c3c88422"
      },
      "outputs": [],
      "source": [
        "# uncomment if using colab; if running locally it's recommended to install via conda\n",
        "# !pip uninstall -y gym\n",
        "# !pip install gymnasium\n",
        "# !pip install gym_super_mario_bros==7.3.0\n",
        "# !pip install wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AoJma_vthJhe"
      },
      "source": [
        "Before we transform the env with wrappers and make our agent, let's take a look at the environment as-is. Below we take some random actions and generate a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PeKvTbRImQ_"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True, render_mode='rgb_array')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "def run_test(env, num_steps=50):\n",
        "    frames = []\n",
        "    out = env.reset()\n",
        "    for _ in range(num_steps):\n",
        "        next_state, reward, done, truncated, info = env.step(action=env.action_space.sample())\n",
        "        frames.append(np.copy(env.render()))\n",
        "    return frames\n",
        "\n",
        "frames = run_test(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAeHNXeZp_4K",
        "outputId": "128bac45-414d-44ff-987c-5d61ab3f1725"
      },
      "outputs": [],
      "source": [
        "print(frames[-1].shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdoA8i-hejy"
      },
      "source": [
        "We can reuse the following code to see what our bot does later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM80quT3QU6r"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "def display_vid(frames, vid_name='./video.mp4', compressed_name='./compressed.mp4', fps=30):\n",
        "\n",
        "    if os.path.isfile(vid_name):\n",
        "      !rm video.mp4\n",
        "    if os.path.isfile(compressed_name):\n",
        "      !rm compressed.mp4\n",
        "\n",
        "    h, w, c = frames[-1].shape\n",
        "    out = cv2.VideoWriter(vid_name, cv2.VideoWriter_fourcc(*'VP90'), fps, (w, h))\n",
        "    for frame in tqdm(frames):\n",
        "        img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        out.write(img)\n",
        "    out.release()\n",
        "\n",
        "    mp4 = open(vid_name,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "          <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "FIJF-g33QpHx",
        "outputId": "89d1c426-82fb-4605-da9e-3162e93329f7"
      },
      "outputs": [],
      "source": [
        "display_vid(frames)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLa9SFHipWS"
      },
      "source": [
        "## Preprocessing Environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mJZj2Eluj2"
      },
      "source": [
        "The DeepMind papers for Atari games transforms the environments to make it easier for the DQN/DDQN to learn. In particular,\n",
        "\n",
        "1. Use only every 4th frame\n",
        "2. Grayscale each image, downscale to 110x84, then crop appropriately to remove unnecesssary info\n",
        "3. Pass a stack of 4 frames so that the model can see trajectory (falling and jumping are different contexts, but may lead to the same observation).\n",
        "\n",
        "\n",
        "We will implement some of these simplifying transformations using Gymnasium's `gym.Wrapper`. Wrapper code is altered from https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkegIeYhm-eD"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import cv2\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._replay_buffer = collections.deque(maxlen=4)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            self._replay_buffer.append(obs.copy())\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, truncated, info\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'rgb_array':\n",
        "            return self._replay_buffer\n",
        "        \n",
        "        return self.env.render(mode=mode)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return ProcessFrame84.process(observation)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        assert frame.size == 240 * 256 * 3, \"Unknown resolution.\"\n",
        "\n",
        "        img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class TransposeObs(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(TransposeObs, self).__init__(env)\n",
        "        w, h, n = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(n, w, h), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset()), {}\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation[0]\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sWxQ2rAjFLX"
      },
      "outputs": [],
      "source": [
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "\n",
        "def preprocessed_env(world=1, level=1):\n",
        "    env = gym_super_mario_bros.make(f'SuperMarioBros-{world}-{level}-v0', apply_api_compatibility=True, render_mode='rgb_array')\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = TransposeObs(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    # we will use complex movement for PPO instead of right only since it's a more advanced algo\n",
        "    return JoypadSpace(env, RIGHT_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Z7ud1a55kJ",
        "outputId": "c7fd887d-fdd7-4313-d20d-aad12765bc5d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "env = preprocessed_env()\n",
        "print('obs space', env.observation_space.shape)\n",
        "print('act space', env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "MN2eDywqR7kC",
        "outputId": "89514658-51f5-469b-9fd4-0f3a8e14077e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(env.reset()[0][-1], cmap='gray')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RT_tBITRA6Ty"
      },
      "source": [
        "We now have a preprocessed environment! We can proceed to working on our Agent."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vCd3JjKZA_s9"
      },
      "source": [
        "## Training Convolutional PPO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q655hjwkBWHy"
      },
      "source": [
        "I will be adapting a previous PPO implementation I made (https://github.com/arth-shukla/ppo-gym-cartpole) to use convolutions to process the observations.\n",
        "\n",
        "I will use the same convolutions as in the original DeepMind papers, which I implemented for a DDQN (https://github.com/arth-shukla/ddqn-mario).\n",
        "\n",
        "1. Conv2d with 32 8x8 kernels, stride 4 + nonlinearlity (we will use ReLU)\n",
        "2. Conv2d with 64 4x4 kernels, stride 2 + nonlinearity\n",
        "3. Conv2d with 64 3x3 kernels, stride 1 + nonlinearity\n",
        "4. FC layer with 512 nodes + nonlinearity\n",
        "5. FC layer with # nodes equal to # actions\n",
        "6. Softmax &rarr; Categorical distr\n",
        "\n",
        "The implementation can be found in `ppo-conv.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pln8Nen6ppv4"
      },
      "outputs": [],
      "source": [
        "from ppo.agent import PPOAgent\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=None,\n",
        "    checkpoint_dir = './checkpoints', pretrained_path=None,\n",
        "    num_games = 2000, avg_over = 100, save_every = 100,\n",
        "    buffer_size = 512,\n",
        "    batch_size = 16,\n",
        "    epochs = 10,\n",
        "    lr = 1e-4,\n",
        "    scheduler_gamma = None,\n",
        "    discount = .9,\n",
        "    gae_lambda = 1.0,\n",
        "    policy_clip = 0.2,\n",
        "    norm_advantage = False,\n",
        "    entropy_coeff = None,\n",
        "    critic_coeff = 0.5,\n",
        "    max_grad_norm = None,\n",
        "    early_stop_kl = None,\n",
        "    embed = 512,\n",
        "):\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "        print('created', checkpoint_dir)\n",
        "\n",
        "    import wandb\n",
        "\n",
        "    if logging and log_init:\n",
        "        wandb.login()\n",
        "        wandb.init(project=wandb_proj, name=run_name, config=dict(\n",
        "            buffer_size = buffer_size,\n",
        "            batch_size = batch_size,\n",
        "            epochs = epochs,\n",
        "            lr = lr,\n",
        "            scheduler_gamma = scheduler_gamma,\n",
        "            discount = discount,\n",
        "            gae_lambda = gae_lambda,\n",
        "            policy_clip = policy_clip,\n",
        "            norm_advantage = norm_advantage,\n",
        "            entropy_coeff = entropy_coeff,\n",
        "            critic_coeff = critic_coeff,\n",
        "            max_grad_norm = max_grad_norm,\n",
        "            early_stop_kl = early_stop_kl,\n",
        "            embed = embed,\n",
        "        ))\n",
        "\n",
        "    env = preprocessed_env()\n",
        "\n",
        "    # create agent (note this is done in train because it needs hyperparams)\n",
        "    agent = PPOAgent(\n",
        "        obs_shape=env.observation_space.shape,\n",
        "        act_shape=env.action_space.shape,\n",
        "        act_n=env.action_space.n,\n",
        "        buffer_size = buffer_size,\n",
        "        batch_size = batch_size,\n",
        "        epochs = epochs,\n",
        "        lr = lr,\n",
        "        scheduler_gamma = scheduler_gamma,\n",
        "        discount = discount,\n",
        "        gae_lambda = gae_lambda,\n",
        "        policy_clip = policy_clip,\n",
        "        norm_advantage = norm_advantage,\n",
        "        entropy_coeff = entropy_coeff,\n",
        "        critic_coeff = critic_coeff,\n",
        "        max_grad_norm = max_grad_norm,\n",
        "        early_stop_kl = early_stop_kl,\n",
        "        embed = embed,\n",
        "    )\n",
        "    \n",
        "    # load model if pretrained\n",
        "    if pretrained_path != None:\n",
        "        agent.load(load_path=pretrained_path)\n",
        "\n",
        "    agent.actor.train()\n",
        "    agent.critic.train()\n",
        "    \n",
        "    # data tracking\n",
        "    score_hist = []\n",
        "    num_steps = 0\n",
        "    learn_steps = 0\n",
        "    avg_score = 0\n",
        "    best_score = 0\n",
        "\n",
        "    # num_games is the number of games to play, where each game ends\n",
        "    # when the agent meets terminal conditions for the env\n",
        "    for ep_num in tqdm(range(num_games)):\n",
        "\n",
        "        obs = env.reset()[0].__array__()\n",
        "        done = False\n",
        "        score = 0\n",
        "        steps = 0\n",
        "\n",
        "        # run loop where model gathers data for \"learn_every\"\n",
        "        # steps then learns using that information\n",
        "        while not done:\n",
        "\n",
        "            # choose action (actor)\n",
        "            action, log_prob, val = agent.act(obs)\n",
        "\n",
        "            # get results of action\n",
        "            next_obs, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            # save data to memory for experience learning\n",
        "            agent.cache(obs, action, log_prob, val, reward, done)\n",
        "\n",
        "            # when buffer full, learn and clear mem\n",
        "            if agent.mem_full():\n",
        "                log_dict = agent.learn(next_obs.__array__())\n",
        "                agent.clear_mem()\n",
        "\n",
        "                if logging:\n",
        "                    log_dict['learn_steps'] = learn_steps\n",
        "                    wandb.log(log_dict)\n",
        "\n",
        "                learn_steps += 1\n",
        "                \n",
        "            # update obs and tracking vars\n",
        "            obs = next_obs.__array__()\n",
        "            score += reward\n",
        "            steps += 1\n",
        "            num_steps += 1\n",
        "\n",
        "        score_hist.append(score)\n",
        "        \n",
        "        if (ep_num % save_every == 0) or (ep_num == num_games - 1):\n",
        "            save_path = Path(checkpoint_dir) / Path(f'mario_ppo_{ep_num}.pt')\n",
        "            agent.save(save_path=save_path)\n",
        "\n",
        "        \n",
        "        if len(score_hist) > avg_over:\n",
        "            avg_score = np.mean(score_hist[-avg_over:])\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                best_save_path = Path(checkpoint_dir) / Path('mario_ppo_best_model.pt')\n",
        "                agent.save(save_path=best_save_path)\n",
        "    \n",
        "        if logging:\n",
        "            log_dict = {\n",
        "                'train/total_rewards': score,\n",
        "                'train/ep_len': steps,\n",
        "                'ep_num': ep_num,\n",
        "            }\n",
        "            if len(score_hist) > avg_over:\n",
        "                log_dict[f'train/avg_reward_last_{avg_over}'] = avg_score\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "    env.close()\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PCLIP = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=f'standard_run_{PCLIP}'\n",
        "train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=name,\n",
        "    checkpoint_dir = f'./checkpoints/{name}', pretrained_path=None,\n",
        "    scheduler_gamma = None,\n",
        "    policy_clip = PCLIP,\n",
        "    entropy_coeff = None,\n",
        "    norm_advantage = False,\n",
        "    max_grad_norm = None,\n",
        "    early_stop_kl = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PCLIP = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=f'standard_run_{PCLIP}'\n",
        "train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=name,\n",
        "    checkpoint_dir = f'./checkpoints/{name}', pretrained_path=None,\n",
        "    scheduler_gamma = None,\n",
        "    policy_clip = PCLIP,\n",
        "    entropy_coeff = None,\n",
        "    norm_advantage = False,\n",
        "    max_grad_norm = None,\n",
        "    early_stop_kl = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=f'early_stop_kl_{PCLIP}'\n",
        "train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=name,\n",
        "    checkpoint_dir = f'./checkpoints/{name}', pretrained_path=None,\n",
        "    scheduler_gamma = None,\n",
        "    policy_clip = PCLIP,\n",
        "    entropy_coeff = None,\n",
        "    norm_advantage = False,\n",
        "    max_grad_norm = None,\n",
        "    early_stop_kl = 0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=f'norms_{PCLIP}'\n",
        "train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=name,\n",
        "    checkpoint_dir = f'./checkpoints/{name}', pretrained_path=None,\n",
        "    scheduler_gamma = None,\n",
        "    policy_clip = PCLIP,\n",
        "    entropy_coeff = None,\n",
        "    norm_advantage = True,\n",
        "    max_grad_norm = 0.5,\n",
        "    early_stop_kl = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=f'entropy_{PCLIP}'\n",
        "train(\n",
        "    logging=True, log_init=True, wandb_proj='Mario-PPO', run_name=name,\n",
        "    checkpoint_dir = f'./checkpoints/{name}', pretrained_path=None,\n",
        "    scheduler_gamma = None,\n",
        "    policy_clip = PCLIP,\n",
        "    entropy_coeff = 0.01,\n",
        "    norm_advantage = False,\n",
        "    max_grad_norm = None,\n",
        "    early_stop_kl = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import wandb\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D7suegWppv4"
      },
      "outputs": [],
      "source": [
        "# for lr in [5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-4]:\n",
        "#     train(logging=True, log_init=True, lr=lr, wandb_proj='Mario-PPO', num_games=100, avg_over=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fwZRdwmKppv5"
      },
      "source": [
        "## Let's Watch Our Bot Play!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIaLmkKmppv4"
      },
      "outputs": [],
      "source": [
        "# def run_level(pretrained_path=None):\n",
        "#     with torch.no_grad():\n",
        "#         frames = []\n",
        "        \n",
        "#         env = preprocessed_env()\n",
        "\n",
        "#         agent = PPOAgent(\n",
        "#             obs_shape=env.observation_space.shape,\n",
        "#             act_shape=env.action_space.shape,\n",
        "#             act_n=env.action_space.n,\n",
        "#         )\n",
        "\n",
        "#         if pretrained_path != None:\n",
        "#             agent.load(pretrained_path)\n",
        "\n",
        "#         agent.actor.eval()\n",
        "#         agent.critic.eval()\n",
        "        \n",
        "#         # data tracking\n",
        "#         score_hist = []\n",
        "#         avg_score = 0\n",
        "#         num_steps = 0\n",
        "\n",
        "#         obs = env.reset()[0].__array__()\n",
        "#         done = False\n",
        "#         score = 0\n",
        "#         steps = 0\n",
        "\n",
        "#         # run loop where model gathers data for \"learn_every\"\n",
        "#         # steps then learns using that information\n",
        "#         while not done:\n",
        "\n",
        "#             # choose action (actor)\n",
        "#             action, log_prob, val = agent.act(obs)\n",
        "\n",
        "#             # get results of action\n",
        "#             next_obs, reward, done, _, _ = env.step(action)\n",
        "\n",
        "#             # save data to memory for experience learning\n",
        "#             agent.cache(obs, action, log_prob, val, reward, done)\n",
        "\n",
        "#             # when buffer full, learn and clear mem\n",
        "#             if agent.mem_full():\n",
        "#                 agent.learn(next_obs.__array__())\n",
        "#                 agent.clear_mem()\n",
        "                \n",
        "#             # update obs and tracking vars\n",
        "#             obs = next_obs.__array__()\n",
        "#             score += reward\n",
        "#             steps += 1\n",
        "#             num_steps += 1\n",
        "\n",
        "#             for x in env.render():\n",
        "#                 frames.append(x.copy())\n",
        "        \n",
        "#         env.close()\n",
        "\n",
        "#         return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SlNcbpppv5",
        "outputId": "3a7139c3-df5d-42d8-a4ac-57b8a286efa5"
      },
      "outputs": [],
      "source": [
        "# frames = run_level(pretrained_path='./checkpoints/smaller_p_clip/mario_ppo_1800.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "alX-AQ6Tppv5"
      },
      "source": [
        "Let's watch our DDQN play Mario! Note that it might still lose sometimes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "hDdVp210ppv5",
        "outputId": "d10045db-6f16-4c03-8d0b-117d0ab6e57a"
      },
      "outputs": [],
      "source": [
        "# display_vid(frames, fps=60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
