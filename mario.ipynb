{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vsu3DASF9oOZ"
      },
      "source": [
        "# DDQN to Beat Mario"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TmJC6m4o-bg3"
      },
      "source": [
        "## Background\n",
        "\n",
        "In 2013, DeepMind made the Deep Q Network to play Atari Games (https://arxiv.org/pdf/1312.5602.pdf). This DQN was able to learn to play simple Atari games at human level, kickstarting the past decade of DeepRL research.\n",
        "\n",
        "In 2015, DeepMind published another paper which played even more Atari Games (https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). This new model included a target network to reduce Q-value overestimation in the previous DQN model. This model is sometimes called DDQNs, and it's what we'll implement here to beat Mario!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmHHVWJ91H5"
      },
      "source": [
        "## Setup and Some Exploration"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EK8eCfnF-Hpu"
      },
      "source": [
        "First, we install gymnasium and the Mario Env.\n",
        "\n",
        "The Mario env can be found here: https://pypi.org/project/gym-super-mario-bros/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97LB2zyA90n1",
        "outputId": "ace16065-3afb-4b63-a812-3412c3c88422"
      },
      "outputs": [],
      "source": [
        "# uncomment if using colab; if running locally it's recommended to install via conda\n",
        "# !pip uninstall -y gym\n",
        "# !pip install gymnasium\n",
        "# !pip install gym_super_mario_bros==7.3.0\n",
        "# !pip install wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AoJma_vthJhe"
      },
      "source": [
        "Before we transform the env with wrappers and make our agent, let's take a look at the environment as-is. Below we take some random actions and generate a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5PeKvTbRImQ_"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True, render_mode='rgb_array')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "def run_test(env, num_steps=50):\n",
        "    frames = []\n",
        "    out = env.reset()\n",
        "    for _ in range(num_steps):\n",
        "        next_state, reward, done, truncated, info = env.step(action=env.action_space.sample())\n",
        "        frames.append(np.copy(env.render()))\n",
        "    return frames\n",
        "\n",
        "frames = run_test(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAeHNXeZp_4K",
        "outputId": "128bac45-414d-44ff-987c-5d61ab3f1725"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "print(frames[-1].shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rmdoA8i-hejy"
      },
      "source": [
        "We can reuse the following code to see what our bot does later on:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "mM80quT3QU6r"
      },
      "outputs": [],
      "source": [
        "# from IPython.display import HTML\n",
        "# from base64 import b64encode\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import cv2\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# def display_vid(frames, vid_name='./video.mp4', compressed_name='./compressed.mp4', fps=30):\n",
        "\n",
        "#     if os.path.isfile(vid_name):\n",
        "#       !rm video.mp4\n",
        "#     if os.path.isfile(compressed_name):\n",
        "#       !rm compressed.mp4\n",
        "\n",
        "#     h, w, c = frames[-1].shape\n",
        "#     out = cv2.VideoWriter(vid_name, cv2.VideoWriter_fourcc(*'VP90'), fps, (w, h))\n",
        "#     for frame in tqdm(frames):\n",
        "#         img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "#         out.write(img)\n",
        "#     out.release()\n",
        "\n",
        "#     os.system(f\"ffmpeg -i {vid_name} -vcodec libx264 {compressed_name}\")\n",
        "\n",
        "#     mp4 = open(vid_name,'rb').read()\n",
        "#     data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "#     return HTML(\"\"\"\n",
        "#     <video width=400 controls>\n",
        "#           <source src=\"%s\" type=\"video/mp4\">\n",
        "#     </video>\n",
        "#     \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "FIJF-g33QpHx",
        "outputId": "89d1c426-82fb-4605-da9e-3162e93329f7"
      },
      "outputs": [],
      "source": [
        "# display_vid(frames)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLa9SFHipWS"
      },
      "source": [
        "## Preprocessing Environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mJZj2Eluj2"
      },
      "source": [
        "The DeepMind papers for Atari games transforms the environments to make it easier for the DQN/DDQN to learn. In particular,\n",
        "\n",
        "1. Use only every 4th frame\n",
        "2. Grayscale each image, downscale to 110x84, then crop appropriately to remove unnecesssary info\n",
        "3. Pass a stack of 4 frames so that the model can see trajectory (falling and jumping are different contexts, but may lead to the same observation).\n",
        "\n",
        "\n",
        "We will implement some of these simplifying transformations using Gymnasium's `gym.Wrapper`. Wrapper code is altered from https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xkegIeYhm-eD"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import cv2\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._replay_buffer = collections.deque(maxlen=4)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, truncated, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            self._replay_buffer.append(obs.copy())\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, truncated, info\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'rgb_array':\n",
        "            return self._replay_buffer\n",
        "        \n",
        "        return self.env.render(mode=mode)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return ProcessFrame84.process(observation)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        assert frame.size == 240 * 256 * 3, \"Unknown resolution.\"\n",
        "\n",
        "        img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class TransposeObs(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(TransposeObs, self).__init__(env)\n",
        "        w, h, n = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(n, w, h), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset()), {}\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation[0]\n",
        "        return self.buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7sWxQ2rAjFLX"
      },
      "outputs": [],
      "source": [
        "from gym.wrappers import FrameStack\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "\n",
        "def preprocessed_env():\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", apply_api_compatibility=True, render_mode='rgb_array')\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = TransposeObs(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    env = ScaledFloatFrame(env)\n",
        "    # we will use complex movement for PPO instead of right only since it's a more advanced algo\n",
        "    return JoypadSpace(env, RIGHT_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Z7ud1a55kJ",
        "outputId": "c7fd887d-fdd7-4313-d20d-aad12765bc5d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obs space (4, 84, 84)\n",
            "act space 5\n"
          ]
        }
      ],
      "source": [
        "env = preprocessed_env()\n",
        "print('obs space', env.observation_space.shape)\n",
        "print('act space', env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discrete(5)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "MN2eDywqR7kC",
        "outputId": "89514658-51f5-469b-9fd4-0f3a8e14077e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x16b69de37d0>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqRElEQVR4nO3df3RU9Z3/8Vd+kEkwZGKCmSQlgeBCgwj+4EcIsKvFdFOWdaHkuLWHblE5pdqgAqf+yCp0tWKo7RaKJ+LqYSOeQlk5W2mxR6iNgscl/IrVitQIAhKFGfyVTIjkh8n9/uHXqffegTDJxE8mPB/n3HN83/ncOx8+jPPizr2fe+Msy7IEAMBXLN50BwAAFyYCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgRJ8FUFVVlUaMGKHk5GQVFRVp7969ffVWAIAYFNcX94L7n//5H33/+9/X448/rqKiIq1evVqbN29WfX29srKyzrltV1eXTpw4oSFDhiguLi7aXQMA9DHLstTc3Kzc3FzFx5/jOMfqA5MnT7bKy8tDdWdnp5Wbm2tVVlZ2u21DQ4MliYWFhYUlxpeGhoZzft8nKsra29tVV1enioqK0Lr4+HiVlJSotrbW1b6trU1tbW2h2vr/B2Q//vGP5fF4ot09AEAfa2tr0y9+8QsNGTLknO2iHkAffvihOjs75fP5bOt9Pp/eeustV/vKyko98MADrvUej0fJycnR7h4A4CvS3WkU41fBVVRUqKmpKbQ0NDSY7hIA4CsQ9SOgoUOHKiEhQYFAwLY+EAgoOzvb1d7j8fBTGwBcgKJ+BJSUlKQJEyaopqYmtK6rq0s1NTUqLi6O9tsBAGJU1I+AJGnp0qWaP3++Jk6cqMmTJ2v16tVqaWnRzTff3BdvBwCIQX0SQN/5znf0wQcfaPny5fL7/bryyiu1bds214UJAIALV58EkCQtWrRIixYt6qvdAwBinPGr4AAAFyYCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARfTYRFYB5zc3NrnWW4yHIra2trjaXXHKJrebpxOgLHAEBAIwggAAARhBAAAAjOAcEDCDO8zkZGRmuNvfcc4+tPnbsmKvNXXfdZasvv/xyW52QkNDDHgJ/wxEQAMAIAggAYAQBBAAwgnNAwADS0tJiq7/97W+72gSDQVvt8XhcbR566CFbvWrVKls9bNiwnnYRCOEICABgBAEEADCCAAIAGEEAAQCM4CIEIIYdPnzYVl955ZW2uqCgwLVNfLz9353hJqvm5OTY6rS0NFvd2Njo2iY9Pf0cPQXcOAICABhBAAEAjCCAAABGcA4IiBEffPCBa93NN99sq4uKimx1cnJyVN570qRJtvrVV191tfn4449tdbhzS8CXcQQEADCCAAIAGEEAAQCMIIAAAEZwEQLQT3V0dNjq3NxcV5trrrnmK+lLWVmZrS4uLna1ufvuu211uAsgBg8eHN2OIaZxBAQAMIIAAgAYEXEAvfzyy7r++uuVm5uruLg4bdmyxfa6ZVlavny5cnJylJKSopKSEh06dCha/QUADBARnwNqaWnRFVdcoVtuuUVz5851vf7II49ozZo1Wr9+vQoKCrRs2TKVlpbq4MGDUZsUB1wIjhw5YqufeOIJQz1x38A03BNRf/jDH9rqrVu3utpwDghfFnEAzZw5UzNnzgz7mmVZWr16te6//37Nnj1bkvT000/L5/Npy5YtuvHGG3vXWwDAgBHVc0BHjx6V3+9XSUlJaJ3X61VRUZFqa2vDbtPW1qZgMGhbAAADX1QDyO/3S5J8Pp9tvc/nC73mVFlZKa/XG1ry8vKi2SUAQD9lfB5QRUWFli5dGqqDwSAhBEgaPXq0rX700UddbW666SZb7fV6bbXH43Ft09nZaas/+eQTV5uhQ4eebzdDtm/fbqt5QB26E9UjoOzsbElSIBCwrQ8EAqHXnDwej9LS0mwLAGDgi2oAFRQUKDs7WzU1NaF1wWBQe/bsCTtzGgBw4Yr4J7jTp0/bnkN/9OhRvfbaa8rIyFB+fr4WL16shx56SKNGjQpdhp2bm6s5c+ZEs98AgBgXcQDt379f3/jGN0L1F+dv5s+fr6eeekp33323WlpatHDhQjU2Nmr69Onatm0bc4AAADYRB9C1114ry7LO+npcXJwefPBBPfjgg73qGHChi4uLs9XhriR94403bLXzooPMzEzXNidOnLDVqamprjbNzc22Oicnx1aH+wflt771LVv9xz/+0dWGf4jiy7gXHADACAIIAGAEAQQAMML4RFQA5ycjI8O1bt26dbbaeb/FrKws1zbO80Th9tve3m6r33//fVv98ccfu7bZsGGDrWZCObrDERAAwAgCCABgBAEEADCCAAIAGMFFCEAMu+yyy2z1H/7wB1v98ssvu7Zpamqy1eEmll9yySW2+tixY7Y6JSXFtc3IkSPP2VfAiSMgAIARBBAAwAgCCABgBOeAgAHkfCZ/9uRJpWPHju1Bb4Bz4wgIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIyIKIAqKys1adIkDRkyRFlZWZozZ47q6+ttbVpbW1VeXq7MzEylpqaqrKxMgUAgqp0GAMS+iAJo586dKi8v1+7du/XCCy+oo6ND//iP/6iWlpZQmyVLlmjr1q3avHmzdu7cqRMnTmju3LlR7zgAILYlRtJ427Zttvqpp55SVlaW6urq9A//8A9qamrSunXrtHHjRs2YMUOSVF1drTFjxmj37t2aMmVK9HoOAIhpvToH1NTUJEnKyMiQJNXV1amjo0MlJSWhNoWFhcrPz1dtbW3YfbS1tSkYDNoWAMDA1+MA6urq0uLFizVt2jRdfvnlkiS/36+kpCSlp6fb2vp8Pvn9/rD7qayslNfrDS15eXk97RIAIIb0OIDKy8t14MABbdq0qVcdqKioUFNTU2hpaGjo1f4AALEhonNAX1i0aJGee+45vfzyyxo2bFhofXZ2ttrb29XY2Gg7CgoEAsrOzg67L4/HI4/H05NuAABiWERHQJZladGiRXr22Wf14osvqqCgwPb6hAkTNGjQINXU1ITW1dfX6/jx4youLo5OjwEAA0JER0Dl5eXauHGjfve732nIkCGh8zper1cpKSnyer1asGCBli5dqoyMDKWlpen2229XcXExV8ABAGwiCqC1a9dKkq699lrb+urqat10002SpFWrVik+Pl5lZWVqa2tTaWmpHnvssah0FgAwcEQUQJZlddsmOTlZVVVVqqqq6nGnAAADH/eCAwAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYQQABAIwggAAARhBAAAAjCCAAgBEEEADACAIIAGAEAQQAMIIAAgAYEVEArV27VuPHj1daWprS0tJUXFys559/PvR6a2urysvLlZmZqdTUVJWVlSkQCES90wCA2BdRAA0bNkwrV65UXV2d9u/frxkzZmj27Nl68803JUlLlizR1q1btXnzZu3cuVMnTpzQ3Llz+6TjAIDYlhhJ4+uvv95Wr1ixQmvXrtXu3bs1bNgwrVu3Ths3btSMGTMkSdXV1RozZox2796tKVOmRK/XAICY1+NzQJ2dndq0aZNaWlpUXFysuro6dXR0qKSkJNSmsLBQ+fn5qq2tPet+2traFAwGbQsAYOCLOIDeeOMNpaamyuPx6NZbb9Wzzz6ryy67TH6/X0lJSUpPT7e19/l88vv9Z91fZWWlvF5vaMnLy4v4DwEAiD0RB9DXv/51vfbaa9qzZ49uu+02zZ8/XwcPHuxxByoqKtTU1BRaGhoaerwvAEDsiOgckCQlJSXp7/7u7yRJEyZM0L59+/SrX/1K3/nOd9Te3q7GxkbbUVAgEFB2dvZZ9+fxeOTxeCLvOQAgpvV6HlBXV5fa2to0YcIEDRo0SDU1NaHX6uvrdfz4cRUXF/f2bQAAA0xER0AVFRWaOXOm8vPz1dzcrI0bN2rHjh3avn27vF6vFixYoKVLlyojI0NpaWm6/fbbVVxczBVwAACXiALo1KlT+v73v6+TJ0/K6/Vq/Pjx2r59u775zW9KklatWqX4+HiVlZWpra1NpaWleuyxx/qk4wCA2BZRAK1bt+6crycnJ6uqqkpVVVW96hQAYODjXnAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACMIIACAEQQQAMAIAggAYAQBBAAwggACABhBAAEAjCCAAABGEEAAACN6FUArV65UXFycFi9eHFrX2tqq8vJyZWZmKjU1VWVlZQoEAr3tJwBggOlxAO3bt0//9V//pfHjx9vWL1myRFu3btXmzZu1c+dOnThxQnPnzu11RwEAA0uPAuj06dOaN2+ennzySV188cWh9U1NTVq3bp1++ctfasaMGZowYYKqq6u1a9cu7d69O2qdBgDEvh4FUHl5uWbNmqWSkhLb+rq6OnV0dNjWFxYWKj8/X7W1tWH31dbWpmAwaFsAAANfYqQbbNq0Sa+++qr27dvnes3v9yspKUnp6em29T6fT36/P+z+Kisr9cADD0TaDQBAjIvoCKihoUF33nmnNmzYoOTk5Kh0oKKiQk1NTaGloaEhKvsFAPRvEQVQXV2dTp06pauvvlqJiYlKTEzUzp07tWbNGiUmJsrn86m9vV2NjY227QKBgLKzs8Pu0+PxKC0tzbYAAAa+iH6Cu+666/TGG2/Y1t18880qLCzUPffco7y8PA0aNEg1NTUqKyuTJNXX1+v48eMqLi6OXq8BADEvogAaMmSILr/8ctu6iy66SJmZmaH1CxYs0NKlS5WRkaG0tDTdfvvtKi4u1pQpU6LXawBAzIv4IoTurFq1SvHx8SorK1NbW5tKS0v12GOPRfttAAAxrtcBtGPHDludnJysqqoqVVVV9XbXAIABjHvBAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMiCiA/uM//kNxcXG2pbCwMPR6a2urysvLlZmZqdTUVJWVlSkQCES90wCA2BfxEdDYsWN18uTJ0PLKK6+EXluyZIm2bt2qzZs3a+fOnTpx4oTmzp0b1Q4DAAaGxIg3SExUdna2a31TU5PWrVunjRs3asaMGZKk6upqjRkzRrt379aUKVN631sAwIAR8RHQoUOHlJubq5EjR2revHk6fvy4JKmurk4dHR0qKSkJtS0sLFR+fr5qa2vPur+2tjYFg0HbAgAY+CIKoKKiIj311FPatm2b1q5dq6NHj+rv//7v1dzcLL/fr6SkJKWnp9u28fl88vv9Z91nZWWlvF5vaMnLy+vRHwQAEFsi+glu5syZof8eP368ioqKNHz4cD3zzDNKSUnpUQcqKiq0dOnSUB0MBgkhALgA9Ooy7PT0dI0ePVqHDx9Wdna22tvb1djYaGsTCATCnjP6gsfjUVpamm0BAAx8vQqg06dP65133lFOTo4mTJigQYMGqaamJvR6fX29jh8/ruLi4l53FAAwsET0E9yPf/xjXX/99Ro+fLhOnDihn/zkJ0pISNB3v/tdeb1eLViwQEuXLlVGRobS0tJ0++23q7i4mCvgAAAuEQXQe++9p+9+97v66KOPdMkll2j69OnavXu3LrnkEknSqlWrFB8fr7KyMrW1tam0tFSPPfZYn3QcABDbIgqgTZs2nfP15ORkVVVVqaqqqledAgAMfNwLDgBgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgRMQB9P777+t73/ueMjMzlZKSonHjxmn//v2h1y3L0vLly5WTk6OUlBSVlJTo0KFDUe00ACD2RRRAn3zyiaZNm6ZBgwbp+eef18GDB/Wf//mfuvjii0NtHnnkEa1Zs0aPP/649uzZo4suukilpaVqbW2NeucBALErMZLGP/vZz5SXl6fq6urQuoKCgtB/W5al1atX6/7779fs2bMlSU8//bR8Pp+2bNmiG2+8MUrdBgDEuoiOgH7/+99r4sSJuuGGG5SVlaWrrrpKTz75ZOj1o0ePyu/3q6SkJLTO6/WqqKhItbW1YffZ1tamYDBoWwAAA19EAXTkyBGtXbtWo0aN0vbt23Xbbbfpjjvu0Pr16yVJfr9fkuTz+Wzb+Xy+0GtOlZWV8nq9oSUvL68nfw4AQIyJKIC6urp09dVX6+GHH9ZVV12lhQsX6gc/+IEef/zxHnegoqJCTU1NoaWhoaHH+wIAxI6IzgHl5OTosssus60bM2aM/vd//1eSlJ2dLUkKBALKyckJtQkEArryyivD7tPj8cjj8UTSjX7v3Xffda07cOCAre7o6HC1GT16tK0uLCy01fHxXDUPYOCI6Btt2rRpqq+vt617++23NXz4cEmfX5CQnZ2tmpqa0OvBYFB79uxRcXFxFLoLABgoIjoCWrJkiaZOnaqHH35Y//qv/6q9e/fqiSee0BNPPCFJiouL0+LFi/XQQw9p1KhRKigo0LJly5Sbm6s5c+b0Rf8BADEqogCaNGmSnn32WVVUVOjBBx9UQUGBVq9erXnz5oXa3H333WppadHChQvV2Nio6dOna9u2bUpOTo565wEAsSvOsizLdCe+LBgMyuv16r777ouZ0Nq1a5etbmlpcbXJysqy1QkJCa427733nq12nieaNWuWaxvOCwHob1pbW7VixQo1NTUpLS3trO349gIAGEEAAQCMIIAAAEZEdBECPnfkyBFb7TwP8+U5UJFwngO66KKLbPWX7zr+hcmTJ/fovQDANI6AAABGEEAAACMIIACAEQQQAMAILkLoRrhJpceOHbPVzkmm5yPcDUvPnDljq1NTU231Z5991u1+vrgvHwD0dxwBAQCMIIAAAEYQQAAAIzgH5OC8N+u+fftcbXpyzsfJOelUksaOHWurMzIybHViovuv6/Dhw7b6kksucbUZPHhwT7oIAH2KIyAAgBEEEADACAIIAGAEAQQAMIKLEBwOHDhgq52TQaPl61//umvdxRdfbKvDPTXVyefz2epwF01cc801EfYOAPoeR0AAACMIIACAEQQQAMCIC/ocUCAQcK1rbGy01c7zMtEydOjQPtmv8ymqkvTmm2/aaueEVwAwgSMgAIARBBAAwAgCCABgBAEEADDigroIoaOjw1Y7J51K7omdsSY5Odm17sMPP7TVH330ka3OzMzs0z4BQDgcAQEAjCCAAABGEEAAACMuqHNAzht1hnt66EDkPMezf/9+W11aWvpVdgcAJHEEBAAwhAACABgRUQCNGDFCcXFxrqW8vFyS1NraqvLycmVmZio1NVVlZWVh77cGAEBE54D27dunzs7OUH3gwAF985vf1A033CBJWrJkif7whz9o8+bN8nq9WrRokebOnav/+7//i26vz9M777xjq+Pj489Zn6+dO3fa6kmTJtnqwYMH92i/cXFxtjox0f7X8+Wx/8If//hHWz1jxgxXG+ef82tf+5qt3rVrl2ubqVOnnruzQIxraGiw1V1dXa42zv/nRo4c2e1+g8GgrT558mS324wePdq1zvl90N7ebquPHj3a7X7z8vJc63r6/dQXIgog50n7lStX6tJLL9U111yjpqYmrVu3Ths3bgx9CVZXV2vMmDHavXu3pkyZEr1eAwBiXo/PAbW3t+vXv/61brnlFsXFxamurk4dHR0qKSkJtSksLFR+fr5qa2vPup+2tjYFg0HbAgAY+HocQFu2bFFjY6NuuukmSZLf71dSUpLS09Nt7Xw+n/x+/1n3U1lZKa/XG1rCHTICAAaeHgfQunXrNHPmTOXm5vaqAxUVFWpqagotzt9lAQADU48mor777rv605/+pN/+9rehddnZ2Wpvb1djY6PtKCgQCCg7O/us+/J4PPJ4PD3phs3p06fD9vPLsrKyorJf5xHdmTNnbHVPT/J5vV5bPWzYMFv96aefurZxnpj87LPPXG2SkpIi7suxY8ds9YgRIyLeB9BfOL8LJOmKK66w1UVFRa42NTU1tvrtt992tXH+I9z5fXDHHXe4tjlx4oStrq6udrW56qqrbLXzz3DLLbe4tklNTbXVK1ascLUpLCw85zZfpR4dAVVXVysrK0uzZs0KrZswYYIGDRpk+wurr6/X8ePHVVxc3PueAgAGlIiPgLq6ulRdXa358+fbLhP2er1asGCBli5dqoyMDKWlpen2229XcXExV8ABAFwiDqA//elPOn78eNjDv1WrVik+Pl5lZWVqa2tTaWmpHnvssah0FAAwsMRZlmWZ7sSXBYNBeb1e3XfffWEfrnY2L774omtdT875OG3fvt21znluZtSoUba6J+dcJGns2LG2Oj8/31Y3NTW5tjl8+LCtDnfnCeeEtvNx6tQpWx3uKLY/TWgDzuXgwYOudQsXLux2u48//thWh7ui96WXXrLVXz41Ibm/LyT3uWXnwzIl6dlnn7XVY8aMsdXOCfBn24/T888/b6t7eyFZOK2trVqxYoWampqUlpZ21nbcCw4AYAQBBAAwggACABhBAAEAjIjZJ6L+5S9/sdXOSZzREm4SrfNigWj58MMPbbXzIorW1lbXNs6TpD254CAc541n9+7d62pz7bXXRuW9gGh79dVXbXW4u8Q7L+rJyMhwtXH+f3DxxRe72jgnp/p8Plsd7q77zqcUh7vgytmfyZMn2+pwFxw4++e8o364/Tq/Q5y3U5N6/uSA7nAEBAAwggACABhBAAEAjIiJiajOSZHS5/eZ+7Jwv832Z+F+U3X+dtzW1tbtNs42zc3NUeidm/N9JPeE23HjxvXJewPdOXLkiK12TgZ1Tuq+kDmfpLpjxw5b3djY6Nrmsssui+g9mIgKAOjXCCAAgBEEEADAiH47D6irq0tdXV2SpNdff931ek5OzlfdpagKd62986FvzhuNhntw1OjRo231rl27et23cMI9NPCjjz6y1c55TJI0dOjQPukPLlwffPCBa93EiRNtNed8zq6goMBWO8dqzZo1rm2cD9CL1g1MOQICABhBAAEAjCCAAABGEEAAACP67UUIe/fuDd1IL9YvOAjn008/da1zTqYLBoO2uqWlxbVNuBuUflWcN1QMd7GI84alCQkJfdklXADC/b/jvAgB58/5/+Rtt93marN27VpbzUUIAICYRgABAIwggAAARvTbc0BTpkwJ3YzUea5Bktrb22218+akklRYWGirnRPYzud3zH379rnWTZo0yVa/++67tnr48OHd7vfPf/6za51zsqfzYXiDBw92beO8SajzhqaSNGzYMFt9+vRpWx3uRq7OvjjHW3KPeVFRkauN88aGzomz4TjH3DneknTs2DFb7ZzEG05dXZ2tvvLKK11t3nvvPVvtHLtw57D++te/2uq8vDxXG+eYOycih3sgmfPv9nw+487JwNH6jEdjvCX3mHc33pJ7zMNNRHVOlHSeJwo38ds55uFuuOscc+d4S9EZ82h9xp3fK1dccYWrTXdj7pxgLkkjR4601eG+Z778GT/fc70cAQEAjCCAAABGEEAAACMIIACAEf32iaiPPPKIUlJSJLknaErSZ5991u2+nG3CnejtTrjJnxdddNE52zhfDyfcU16dJ/qOHz9uq8OdoHWebH377bddbUpKSmz1tm3bbPU///M/u7ZxjnlPxluKzpiHG89ojLlzvKXuxzzcJEjnmDvHW+p+zPmMf47PeGRtnPrLZ/yzzz5TTU0NT0QFAPRPBBAAwAgCCABgRL+diBqpcJPpkpKSbLXzxpjNzc3d7vf99993rQsEArZ62rRp59FDu3CT3l544QVb7XxyYU+99NJLtjrcRLOecI65c7yl6Iy5c7yl6Iy5c7yl6Iy5c7yl6Iw5n/Gz4zP+uf7yGe/s7Dyv/XAEBAAwggACABgRUQB1dnZq2bJlKigoUEpKii699FL99Kc/1Zev5LYsS8uXL1dOTo5SUlJUUlKiQ4cORb3jAIDYFtE5oJ/97Gdau3at1q9fr7Fjx2r//v26+eab5fV6dccdd0iSHnnkEa1Zs0br169XQUGBli1bptLSUh08eLBH18ufL+fvsJL7BoQ9+V04Pz/fte5b3/qWrX7nnXci3u+gQYNc6xYsWGCrDx48GPF+wykrK7PVO3bsiMp+nWMe7oaP0Rhz53hL0Rlz53hL0Rlz53hL0RlzPuNnx2f8c/3lM97e3h72hstOEQXQrl27NHv2bM2aNUvS53dn/c1vfqO9e/dK+vzoZ/Xq1br//vs1e/ZsSdLTTz8tn8+nLVu26MYbb4zk7QAAA1hEP8FNnTpVNTU1oVmxr7/+ul555RXNnDlTknT06FH5/X7bLFmv16uioiLV1taG3WdbW5uCwaBtAQAMfBEdAd17770KBoMqLCxUQkKCOjs7tWLFCs2bN0+S5Pf7JbmfFeHz+UKvOVVWVuqBBx7oSd8BADEsoiOgZ555Rhs2bNDGjRv16quvav369frFL36h9evX97gDFRUVampqCi0NDQ093hcAIHZEdAR011136d577w2dyxk3bpzeffddVVZWav78+aEneAYCAeXk5IS2CwQCYZ8+KX3+5E3n0zcl6cyZMzrXfVKdNwVsbW11tXE+qbSrq8tWh3vC6Plw3sTwk08+sdUZGRnd7iPcEwOdNwl0HjWGe9KmU3y8+98Uzic2Op8eGe4pj07hbsLoHHPneEvRGfNwN+qMxpg7x1uKzpg7x1uKzpjzGf8cn/Gz6y+f8fO5sasU4RHQp59+6upIQkJC6C+goKBA2dnZqqmpCb0eDAa1Z88eFRcXR/JWAIABLqIjoOuvv14rVqxQfn6+xo4dqz//+c/65S9/qVtuuUWSFBcXp8WLF+uhhx7SqFGjQpdh5+bmas6cOX3RfwBAjIoogB599FEtW7ZMP/rRj3Tq1Cnl5ubqhz/8oZYvXx5qc/fdd6ulpUULFy5UY2Ojpk+frm3btvXpHCAAQOzptw+kmz9/fujGf42Nja52p0+fttXf+MY3XG2OHj1qq503AIyLi3Ntk5hoz+TRo0e72jh/F37rrbdsdbgbFg4ZMsRWh5vQ5pxE5txvuAc7OfsbLugzMzNt9euvv26rw01EdI65c7wl95g7x1vqfsyd/ZfcYx7u3Ed3Y+4cb8k95uEmSnY35uH66xxz53hL3Y85n/HP8Rn/m1j+jPNAOgBAv0YAAQCMIIAAAEYQQAAAI/rtE1EnTZqklJQUSQp7dwTnEwRPnjzpatPS0mKrR40aZavDTexynrwMdwLNud8vJuB+IdxJ3QMHDthq5+2KJPdJ0YsuushWT5o0ybWN8068H3/8satNR0eHrXb2f8qUKa5tnGMe7omNzjF37lfqfsyd4y25xzzcfrsbc+d4S+4xD3fiv7sxD3fnY+eYO8db6n7M+Yx/js/438TyZ7xPJqICABAtBBAAwIh+9xPcF9OSzpw5E1oX7hr59vZ2Wx1uvoOzTVtbm60Ot1/noWW4Ns79ON8nWvuNRl8k9/2hnIfHXx7rs71XuP06xzxcm+7GPNyhvLONcx/h3ita++1uzM+nL+HugdbdmPMZ73lfJD7jkez3q/iMf/Hf3U0z7XcTUd97773zujkeAKB/a2ho0LBhw876er8LoK6uLp04cUJDhgxRc3Oz8vLy1NDQcM7ZtOiZYDDI+PYhxrdvMb59qzfja1mWmpublZubG/bu5V/odz/BxcfHhxLzi8PftLQ0PmB9iPHtW4xv32J8+1ZPx9fr9XbbhosQAABGEEAAACP6dQB5PB795Cc/CfvEVPQe49u3GN++xfj2ra9ifPvdRQgAgAtDvz4CAgAMXAQQAMAIAggAYAQBBAAwggACABjRbwOoqqpKI0aMUHJysoqKirR3717TXYpJlZWVmjRpkoYMGaKsrCzNmTNH9fX1tjatra0qLy9XZmamUlNTVVZWFvbZKDi3lStXKi4uTosXLw6tY2x77/3339f3vvc9ZWZmKiUlRePGjdP+/ftDr1uWpeXLlysnJ0cpKSkqKSnRoUOHDPY4dnR2dmrZsmUqKChQSkqKLr30Uv30pz+13US0T8fX6oc2bdpkJSUlWf/93/9tvfnmm9YPfvADKz093QoEAqa7FnNKS0ut6upq68CBA9Zrr71m/dM//ZOVn59vnT59OtTm1ltvtfLy8qyamhpr//791pQpU6ypU6ca7HXs2bt3rzVixAhr/Pjx1p133hlaz9j2zscff2wNHz7cuummm6w9e/ZYR44csbZv324dPnw41GblypWW1+u1tmzZYr3++uvWv/zLv1gFBQXWmTNnDPY8NqxYscLKzMy0nnvuOevo0aPW5s2brdTUVOtXv/pVqE1fjm+/DKDJkydb5eXlobqzs9PKzc21KisrDfZqYDh16pQlydq5c6dlWZbV2NhoDRo0yNq8eXOozV//+ldLklVbW2uqmzGlubnZGjVqlPXCCy9Y11xzTSiAGNveu+eee6zp06ef9fWuri4rOzvb+vnPfx5a19jYaHk8Hus3v/nNV9HFmDZr1izrlltusa2bO3euNW/ePMuy+n58+91PcO3t7aqrq1NJSUloXXx8vEpKSlRbW2uwZwNDU1OTpL89Nriurk4dHR228S4sLFR+fj7jfZ7Ky8s1a9Ys2xhKjG00/P73v9fEiRN1ww03KCsrS1dddZWefPLJ0OtHjx6V3++3jbHX61VRURFjfB6mTp2qmpoavf3225I+f3z4K6+8opkzZ0rq+/Htd3fD/vDDD9XZ2el6trnP59Nbb71lqFcDQ1dXlxYvXqxp06bp8ssvlyT5/X4lJSUpPT3d1tbn88nv9xvoZWzZtGmTXn31Ve3bt8/1GmPbe0eOHNHatWu1dOlS/fu//7v27dunO+64Q0lJSZo/f35oHMN9XzDG3bv33nsVDAZVWFiohIQEdXZ2asWKFZo3b54k9fn49rsAQt8pLy/XgQMH9Morr5juyoDQ0NCgO++8Uy+88IKSk5NNd2dA6urq0sSJE/Xwww9Lkq666iodOHBAjz/+uObPn2+4d7HvmWee0YYNG7Rx40aNHTtWr732mhYvXqzc3NyvZHz73U9wQ4cOVUJCgutKoUAgoOzsbEO9in2LFi3Sc889p5deesn2hMLs7Gy1t7ersbHR1p7x7l5dXZ1OnTqlq6++WomJiUpMTNTOnTu1Zs0aJSYmyufzMba9lJOTo8suu8y2bsyYMTp+/LgkhcaR74ueueuuu3Tvvffqxhtv1Lhx4/Rv//ZvWrJkiSorKyX1/fj2uwBKSkrShAkTVFNTE1rX1dWlmpoaFRcXG+xZbLIsS4sWLdKzzz6rF198UQUFBbbXJ0yYoEGDBtnGu76+XsePH2e8u3HdddfpjTfe0GuvvRZaJk6cqHnz5oX+m7HtnWnTprmmDbz99tsaPny4JKmgoEDZ2dm2MQ4Gg9qzZw9jfB4+/fRT1xNLExIS1NXVJekrGN9eX8bQBzZt2mR5PB7rqaeesg4ePGgtXLjQSk9Pt/x+v+muxZzbbrvN8nq91o4dO6yTJ0+Glk8//TTU5tZbb7Xy8/OtF1980dq/f79VXFxsFRcXG+x17PryVXCWxdj21t69e63ExERrxYoV1qFDh6wNGzZYgwcPtn7961+H2qxcudJKT0+3fve731l/+ctfrNmzZ3MZ9nmaP3++9bWvfS10GfZvf/tba+jQodbdd98datOX49svA8iyLOvRRx+18vPzraSkJGvy5MnW7t27TXcpJkkKu1RXV4fanDlzxvrRj35kXXzxxdbgwYOtb3/729bJkyfNdTqGOQOIse29rVu3Wpdffrnl8XiswsJC64knnrC93tXVZS1btszy+XyWx+OxrrvuOqu+vt5Qb2NLMBi07rzzTis/P99KTk62Ro4cad13331WW1tbqE1fji/PAwIAGNHvzgEBAC4MBBAAwAgCCABgBAEEADCCAAIAGEEAAQCMIIAAAEYQQAAAIwggAIARBBAAwAgCCABgxP8DRQB83bfraFcAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(env.reset()[0][-1], cmap='gray')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RT_tBITRA6Ty"
      },
      "source": [
        "We now have a preprocessed environment! We can proceed to working on our Agent."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vCd3JjKZA_s9"
      },
      "source": [
        "## Training Convolutional PPO"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q655hjwkBWHy"
      },
      "source": [
        "I will be adapting a previous PPO implementation I made (https://github.com/arth-shukla/ppo-gym-cartpole) to use convolutions to process the observations.\n",
        "\n",
        "I will use the same convolutions as in the original DeepMind papers, which I implemented for a DDQN (https://github.com/arth-shukla/ddqn-mario).\n",
        "\n",
        "1. Conv2d with 32 8x8 kernels, stride 4 + nonlinearlity (we will use ReLU)\n",
        "2. Conv2d with 64 4x4 kernels, stride 2 + nonlinearity\n",
        "3. Conv2d with 64 3x3 kernels, stride 1 + nonlinearity\n",
        "4. FC layer with 512 nodes + nonlinearity\n",
        "5. FC layer with # nodes equal to # actions\n",
        "6. Softmax &rarr; Categorical distr\n",
        "\n",
        "The implementation can be found in `ppo-conv.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Pln8Nen6ppv4"
      },
      "outputs": [],
      "source": [
        "from ppo.agent import PPOAgent\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def train(\n",
        "        logging=False, log_init=False, wandb_proj='Mario-PPO', checkpoint_dir = './checkpoints',\n",
        "        pretrained_path=None,\n",
        "        num_games=300, avg_over = 500, save_every = 100,\n",
        "        buffer_size = 2048,\n",
        "        batch_size = 64,\n",
        "        epochs = 4,\n",
        "        lr = 2.5e-4,\n",
        "        scheduler_gamma = None,\n",
        "        discount = .99,\n",
        "        gae_lambda = 0.95,\n",
        "        policy_clip = 0.2,\n",
        "        norm_advantage = False,\n",
        "        entropy_coeff = 0.01,\n",
        "        critic_coeff = 0.5,\n",
        "        max_grad_norm = 0.5,\n",
        "        early_stop_kl = None,\n",
        "        embed = 512,\n",
        "    ):\n",
        "    import wandb\n",
        "\n",
        "    if logging and log_init:\n",
        "        wandb.login()\n",
        "        wandb.init(project=wandb_proj, config=dict(\n",
        "            buffer_size = buffer_size,\n",
        "            batch_size = batch_size,\n",
        "            epochs = epochs,\n",
        "            lr = lr,\n",
        "            scheduler_gamma = scheduler_gamma,\n",
        "            discount = discount,\n",
        "            gae_lambda = gae_lambda,\n",
        "            policy_clip = policy_clip,\n",
        "            norm_advantage = norm_advantage,\n",
        "            entropy_coeff = entropy_coeff,\n",
        "            critic_coeff = critic_coeff,\n",
        "            max_grad_norm = max_grad_norm,\n",
        "            early_stop_kl = early_stop_kl,\n",
        "            embed = embed,\n",
        "        ))\n",
        "\n",
        "    env = preprocessed_env()\n",
        "\n",
        "    # create agent (note this is done in train because it needs hyperparams)\n",
        "    agent = PPOAgent(\n",
        "        obs_shape=env.observation_space.shape,\n",
        "        act_shape=env.action_space.shape,\n",
        "        act_n=env.action_space.n,\n",
        "        buffer_size = buffer_size,\n",
        "        batch_size = batch_size,\n",
        "        epochs = epochs,\n",
        "        lr = lr,\n",
        "        scheduler_gamma = scheduler_gamma,\n",
        "        discount = discount,\n",
        "        gae_lambda = gae_lambda,\n",
        "        policy_clip = policy_clip,\n",
        "        norm_advantage = norm_advantage,\n",
        "        entropy_coeff = entropy_coeff,\n",
        "        critic_coeff = critic_coeff,\n",
        "        max_grad_norm = max_grad_norm,\n",
        "        early_stop_kl = early_stop_kl,\n",
        "        embed = embed,\n",
        "    )\n",
        "    \n",
        "    # load model if pretrained\n",
        "    if pretrained_path != None:\n",
        "        agent.load(path=pretrained_path)\n",
        "    \n",
        "    # data tracking\n",
        "    score_hist = []\n",
        "    avg_score = 0\n",
        "    num_steps = 0\n",
        "\n",
        "    # if using kldiv early end, track with this var\n",
        "    early_end = False\n",
        "\n",
        "    # num_games is the number of games to play, where each game ends\n",
        "    # when the agent meets terminal conditions for the env\n",
        "    for ep_num in tqdm(range(num_games)):\n",
        "\n",
        "        obs = env.reset()[0].__array__()\n",
        "        done = False\n",
        "        score = 0\n",
        "        steps = 0\n",
        "\n",
        "        # run loop where model gathers data for \"learn_every\"\n",
        "        # steps then learns using that information\n",
        "        while not done:\n",
        "\n",
        "            # choose action (actor)\n",
        "            action, log_prob, val = agent.act(obs)\n",
        "\n",
        "            # get results of action\n",
        "            next_obs, reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "            # save data to memory for experience learning\n",
        "            obs = torch.tensor(obs)\n",
        "            reward = torch.tensor([reward])\n",
        "            done = torch.tensor([done])\n",
        "            agent.cache(obs, action, log_prob, val, reward, done)\n",
        "\n",
        "            # when buffer full, learn and clear mem\n",
        "            if agent.mem_full():\n",
        "                next_obs_tensor = torch.from_numpy(next_obs.__array__()).to(agent.device)\n",
        "                log_info = agent.learn(next_obs_tensor)\n",
        "                agent.clear_mem()\n",
        "\n",
        "                if logging:\n",
        "                    log_info['num_steps'] = num_steps\n",
        "                    wandb.log(log_info)\n",
        "                \n",
        "            # update obs and tracking vars\n",
        "            obs = next_obs.__array__()\n",
        "            score += reward\n",
        "            steps += 1\n",
        "            num_steps += 1\n",
        "\n",
        "        score_hist.append(score)\n",
        "        \n",
        "        if (ep_num % save_every == 0) or (ep_num == 0):\n",
        "            save_path = Path(checkpoint_dir) / Path(f'mario_ppo_{ep_num}.pt')\n",
        "            agent.save(path=save_path)\n",
        "    \n",
        "        if logging:\n",
        "            log_dict = {\n",
        "                'train/total_rewards': score,\n",
        "                'train/ep_len': steps,\n",
        "                'ep_num': ep_num,\n",
        "            }\n",
        "            if len(score_hist) > avg_over:\n",
        "                log_dict[f'train/avg_reward_last_{avg_over}'] = np.mean(score_hist[-avg_over:])\n",
        "            wandb.log(log_dict)\n",
        "\n",
        "    env.close()\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\arths\\Documents\\GitHub\\ppo-mario\\wandb\\run-20230604_021332-rny30kxa</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/arth-shukla/Mario-PPO/runs/rny30kxa' target=\"_blank\">lyric-voice-20</a></strong> to <a href='https://wandb.ai/arth-shukla/Mario-PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/arth-shukla/Mario-PPO' target=\"_blank\">https://wandb.ai/arth-shukla/Mario-PPO</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/arth-shukla/Mario-PPO/runs/rny30kxa' target=\"_blank\">https://wandb.ai/arth-shukla/Mario-PPO/runs/rny30kxa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|â–‰         | 9/100 [17:39<2:58:32, 117.72s/it]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(logging\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_init\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, wandb_proj\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMario-PPO\u001b[39;49m\u001b[39m'\u001b[39;49m, num_games\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, avg_over\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
            "Cell \u001b[1;32mIn[42], line 123\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(logging, log_init, wandb_proj, checkpoint_dir, pretrained_path, num_games, avg_over, save_every, buffer_size, batch_size, epochs, lr, scheduler_gamma, discount, gae_lambda, policy_clip, norm_advantage, entropy_coeff, critic_coeff, max_grad_norm, early_stop_kl, embed)\u001b[0m\n\u001b[0;32m    120\u001b[0m     num_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    122\u001b[0m score_hist\u001b[39m.\u001b[39mappend(score)\n\u001b[1;32m--> 123\u001b[0m avg_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(score_hist) \u001b[39m<\u001b[39m avg_over \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39;49mmean(score_hist[\u001b[39m-\u001b[39;49mavg_over:])\n\u001b[0;32m    125\u001b[0m \u001b[39mif\u001b[39;00m (ep_num \u001b[39m%\u001b[39m save_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m (ep_num \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[0;32m    126\u001b[0m     save_path \u001b[39m=\u001b[39m Path(checkpoint_dir) \u001b[39m/\u001b[39m Path(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmario_ppo_\u001b[39m\u001b[39m{\u001b[39;00mep_num\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\arths\\anaconda3\\envs\\mario-rl\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3461\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3462\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3464\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   3465\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\arths\\anaconda3\\envs\\mario-rl\\Lib\\site-packages\\numpy\\core\\_methods.py:165\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mean\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 165\u001b[0m     arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    167\u001b[0m     is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39mkeepdims, where\u001b[39m=\u001b[39mwhere)\n",
            "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "train(logging=True, log_init=True, wandb_proj='Mario-PPO', num_games=1000, avg_over=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D7suegWppv4"
      },
      "outputs": [],
      "source": [
        "# for lr in [5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-4]:\n",
        "#     train(logging=True, log_init=True, lr=lr, wandb_proj='Mario-PPO', num_games=100, avg_over=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fwZRdwmKppv5"
      },
      "source": [
        "## Let's Watch Our Bot Play!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIaLmkKmppv4"
      },
      "outputs": [],
      "source": [
        "# def run_level(pretrained_path=None):\n",
        "#     with torch.no_grad():\n",
        "#         frames = []\n",
        "        \n",
        "#         env = preprocessed_env()\n",
        "\n",
        "#         agent = DQNAgent(\n",
        "#             state_space=env.observation_space.shape,\n",
        "#             action_space=env.action_space.n,\n",
        "#             max_memory_size=30000,\n",
        "#             batch_size=32,\n",
        "#             gamma=0.90,\n",
        "#             lr=0.00025,\n",
        "#             exploration_max=1.0,\n",
        "#             exploration_min=0.02,\n",
        "#             exploration_decay=0.99,\n",
        "#             have_mem=False\n",
        "#         )\n",
        "\n",
        "#         if pretrained_path != None:\n",
        "#             agent.load(pretrained_path)\n",
        "\n",
        "#         agent.local_net.eval()\n",
        "\n",
        "#         env.reset()\n",
        "\n",
        "#         state, _ = env.reset()\n",
        "#         state = torch.Tensor([state])\n",
        "#         total_reward = 0\n",
        "#         steps = 0\n",
        "\n",
        "#         total_reward = 0\n",
        "#         while True:\n",
        "#             action = agent.act(state)\n",
        "#             steps += 1\n",
        "\n",
        "#             next_state, reward, terminal, truncated, info = env.step(int(action[0]))\n",
        "\n",
        "#             total_reward += reward\n",
        "\n",
        "#             state = torch.Tensor([next_state])\n",
        "            \n",
        "#             for x in env.render():\n",
        "#                 frames.append(x.copy())\n",
        "\n",
        "#             if terminal:\n",
        "#                 break\n",
        "\n",
        "#         env.close()\n",
        "        \n",
        "#         return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SlNcbpppv5",
        "outputId": "3a7139c3-df5d-42d8-a4ac-57b8a286efa5"
      },
      "outputs": [],
      "source": [
        "# frames = run_level(pretrained_path='./acm_ai_ddqn_checkpoint.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "alX-AQ6Tppv5"
      },
      "source": [
        "Let's watch our DDQN play Mario! Note that it might still lose sometimes..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "hDdVp210ppv5",
        "outputId": "d10045db-6f16-4c03-8d0b-117d0ab6e57a"
      },
      "outputs": [],
      "source": [
        "# display_vid(frames, fps=60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
